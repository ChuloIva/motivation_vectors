{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation Vectors - Llama 3.1 8B Instruct\n",
    "\n",
    "This notebook demonstrates using control vectors with Llama 3.1 8B Instruct to study motivation-related behaviors.\n",
    "\n",
    "Designed to run in Google Colab with the motivation_vectors project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Mount Google Drive (for saving outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the motivation_vectors repository\n",
    "!git clone https://github.com/ChuloIva/motivation_vectors.git\n",
    "%cd motivation_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install torch transformers accelerate bitsandbytes\n",
    "!pip install -e third_party/repeng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Auto-reload and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model: Llama 3.1 8B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# You'll need to set your HuggingFace token here\n",
    "# Get it from: https://huggingface.co/settings/tokens\n",
    "hf_token = \"\"  # Add your token here\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "# Load model with 8-bit quantization for Colab (saves memory)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap model with ControlModel\n",
    "# For 8B model, we use layers 8-24 (roughly middle layers)\n",
    "model = ControlModel(model, list(range(8, 24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load output suffixes from the repeng data files\n",
    "with open(\"third_party/repeng/notebooks/data/all_truncated_outputs.json\") as f:\n",
    "    output_suffixes = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(output_suffixes)} output suffixes for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "def chat_template_parse(resp: str) -> list[dict[str, str]]:\n",
    "    \"\"\"Parse Llama 3.1 chat format back to messages.\"\"\"\n",
    "    resp = resp.strip().removeprefix(\"<|begin_of_text|>\")\n",
    "    messages = []\n",
    "    for part in resp.split(\"<|start_header_id|>\"):\n",
    "        role_and_content = part.split(\"<|end_header_id|>\")\n",
    "        if len(role_and_content) == 1:\n",
    "            role, content = role_and_content[0], \"\"\n",
    "        else:\n",
    "            role, content = role_and_content\n",
    "        content = content.split(\"<|eot_id|>\")[0]\n",
    "        messages.append({\"role\": role.strip(), \"content\": content.strip()})\n",
    "    return messages\n",
    "\n",
    "\n",
    "class HTMLStreamer(TextStreamer):\n",
    "    \"\"\"Streams model output as formatted HTML in notebook.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.display_handle = display(display_id=True)\n",
    "        self.full_text = \"\"\n",
    "\n",
    "    def _is_chinese_char(self, _):\n",
    "        # hack to force token-by-token streaming\n",
    "        return True\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.full_text += text\n",
    "        messages = chat_template_parse(self.full_text)\n",
    "\n",
    "        parts = [\n",
    "            \"<div style='border: 1px solid black; border-radius: 5px; margin-bottom: 5px; padding: 5px;'>\"\n",
    "        ]\n",
    "        for m in messages:\n",
    "            content = (\n",
    "                m[\"content\"]\n",
    "                .replace(\"<\", \"&lt;\")\n",
    "                .replace(\">\", \"&gt;\")\n",
    "                .replace(\"\\n\", \"<br>\")\n",
    "            )\n",
    "            parts.append(f\"<strong>{m['role']}</strong>\")\n",
    "            parts.append(f\"<p>{content}</p>\")\n",
    "        parts.append(\"</div>\")\n",
    "        html = HTML(\"\".join(parts))\n",
    "        self.display_handle.update(html)\n",
    "\n",
    "\n",
    "def generate_with_vector(\n",
    "    input: str,\n",
    "    *vectors,\n",
    "    max_new_tokens: int = 128,\n",
    "    show_baseline: bool = False,\n",
    "    temperature: float = 0.7,\n",
    "):\n",
    "    \"\"\"Generate text with control vectors applied.\n",
    "    \n",
    "    Usage:\n",
    "        generate_with_vector(\"Who am I speaking to?\", vec(\"a cat\") * 0.5)\n",
    "        generate_with_vector(\"Tell me about yourself\", vec(\"motivated\") * 0.7, temperature=1.0)\n",
    "        generate_with_vector(\"What are you?\", vec(\"a cat\") * 0.5 - vec(\"being something\") * 0.3)\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": input},\n",
    "            ],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        ),\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "\n",
    "    def gen(label):\n",
    "        if label:\n",
    "            display(HTML(f\"<h3>{label}</h3>\"))\n",
    "        _ = model.generate(streamer=HTMLStreamer(tokenizer), **input_ids, **settings)\n",
    "\n",
    "    if show_baseline:\n",
    "        model.reset()\n",
    "        gen(\"baseline\")\n",
    "    for vector in vectors:\n",
    "        model.set_control(vector)\n",
    "        gen(\"\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Vector Creation with `vec()`\n",
    "\n",
    "The `vec()` function makes it super easy to create and cache control vectors for any persona!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_persona = \"anything\"\n",
    "\n",
    "\n",
    "def generation_prompt(persona):\n",
    "    \"\"\"Create a generation prompt using the chat template.\"\"\"\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"Please talk about {persona}.\"},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "\n",
    "def train_persona_vector(persona):\n",
    "    \"\"\"Train a control vector for a given persona.\"\"\"\n",
    "    dataset = []\n",
    "    persona_prompt = generation_prompt(persona)\n",
    "    default_prompt = generation_prompt(default_persona)\n",
    "    for suffix in output_suffixes:\n",
    "        dataset.append(\n",
    "            DatasetEntry(\n",
    "                positive=persona_prompt + suffix,\n",
    "                negative=default_prompt + suffix,\n",
    "            )\n",
    "        )\n",
    "    return ControlVector.train(\n",
    "        model, tokenizer, dataset, method=\"pca_center\", batch_size=32\n",
    "    )\n",
    "\n",
    "\n",
    "# Cache for trained vectors\n",
    "cache = {}\n",
    "\n",
    "\n",
    "def vec(persona):\n",
    "    \"\"\"Get or create a cached control vector for a persona.\n",
    "    \n",
    "    Usage:\n",
    "        vec(\"a cat\") * 0.5\n",
    "        vec(\"the Golden Gate Bridge\") * 0.7\n",
    "        vec(\"a cat\") * 0.5 - vec(\"being something\") * 0.3\n",
    "    \"\"\"\n",
    "    if persona not in cache:\n",
    "        print(f\"Training vector for '{persona}'...\")\n",
    "        cache[persona] = train_persona_vector(persona)\n",
    "    return cache[persona]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Testing Examples\n",
    "\n",
    "Try out the simple `vec()` API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple vector application\n",
    "generate_with_vector(\n",
    "    \"Who am I speaking to?\",\n",
    "    vec(\"the Golden Gate Bridge\") * 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Combining vectors with addition/subtraction\n",
    "generate_with_vector(\n",
    "    \"Who am I speaking to? Please describe yourself, including any physical details.\",\n",
    "    vec(\"a cat\") * 0.5 - vec(\"being something\") * 0.3,\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Motivation-specific vectors\n",
    "generate_with_vector(\n",
    "    \"How should I approach my work today?\",\n",
    "    vec(\"extreme motivation and enthusiasm\") * 0.7,\n",
    "    max_new_tokens=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Negative application (removing traits)\n",
    "generate_with_vector(\n",
    "    \"Who am I speaking to? Please describe yourself, including any physical details.\",\n",
    "    vec(\"an artificial intelligence\") * -0.5,\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation-Specific Test Questions\n",
    "\n",
    "A set of questions designed to test motivation-related behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motivation_test_questions = [\n",
    "    \"How should I approach a difficult task that seems overwhelming?\",\n",
    "    \"I'm feeling discouraged about my progress. What should I do?\",\n",
    "    \"How do I stay focused when working on a long-term goal?\",\n",
    "    \"What's the best way to handle setbacks and failures?\",\n",
    "    \"How can I maintain my motivation over time?\",\n",
    "    \"I don't feel like doing my work today. What should I do?\",\n",
    "    \"How do I deal with procrastination?\",\n",
    "    \"What's the point of working hard if success isn't guaranteed?\",\n",
    "    \"How should I prioritize my goals?\",\n",
    "    \"I'm tired and stressed. Should I push through or rest?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Motivation Vectors\n",
    "\n",
    "Try different motivation-related personas on your test questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a motivation vector\n",
    "generate_with_vector(\n",
    "    motivation_test_questions[0],\n",
    "    vec(\"extreme motivation and resilience\") * 0.8,\n",
    "    max_new_tokens=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test growth mindset vs fixed mindset\n",
    "generate_with_vector(\n",
    "    \"I failed at my first attempt. What should I do?\",\n",
    "    vec(\"growth mindset\") * 0.7,\n",
    "    max_new_tokens=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine multiple motivation traits\n",
    "generate_with_vector(\n",
    "    \"I'm working on a big project. How should I approach it?\",\n",
    "    vec(\"goal-oriented focus\") * 0.5 + vec(\"self-confidence\") * 0.4,\n",
    "    max_new_tokens=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Testing (Optional)\n",
    "\n",
    "Run comprehensive tests and save results to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define motivation vectors to test\n",
    "motivation_vectors = [\n",
    "    \"extreme motivation and enthusiasm\",\n",
    "    \"growth mindset\",\n",
    "    \"resilience and perseverance\",\n",
    "    \"goal-oriented focus\",\n",
    "    \"self-confidence and self-efficacy\",\n",
    "]\n",
    "\n",
    "# Test strengths\n",
    "test_strengths = [0.5, 0.7, 1.0]\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"Running comprehensive motivation vector tests...\\n\")\n",
    "\n",
    "for question_idx, question in enumerate(motivation_test_questions):\n",
    "    print(f\"\\nQuestion {question_idx + 1}/{len(motivation_test_questions)}: {question}\")\n",
    "    \n",
    "    for persona in motivation_vectors:\n",
    "        print(f\"  Testing '{persona}'...\")\n",
    "        \n",
    "        for strength in test_strengths:\n",
    "            try:\n",
    "                # Generate response\n",
    "                input_ids = tokenizer(\n",
    "                    tokenizer.apply_chat_template(\n",
    "                        [{\"role\": \"user\", \"content\": question}],\n",
    "                        add_generation_prompt=True,\n",
    "                        tokenize=False,\n",
    "                    ),\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(model.device)\n",
    "                \n",
    "                model.set_control(vec(persona) * strength)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model.generate(\n",
    "                        **input_ids,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        temperature=0.7,\n",
    "                        max_new_tokens=150,\n",
    "                    )\n",
    "                \n",
    "                full_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "                model.reset()\n",
    "                \n",
    "                # Parse the response\n",
    "                messages = chat_template_parse(full_text)\n",
    "                assistant_response = \"\"\n",
    "                for m in messages:\n",
    "                    if m[\"role\"] == \"assistant\":\n",
    "                        assistant_response = m[\"content\"]\n",
    "                        break\n",
    "                \n",
    "                result = {\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"question\": question,\n",
    "                    \"question_index\": question_idx,\n",
    "                    \"persona\": persona,\n",
    "                    \"strength\": strength,\n",
    "                    \"response\": assistant_response,\n",
    "                    \"full_output\": full_text,\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error with '{persona}' at strength {strength}: {e}\")\n",
    "                results.append({\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"question\": question,\n",
    "                    \"question_index\": question_idx,\n",
    "                    \"persona\": persona,\n",
    "                    \"strength\": strength,\n",
    "                    \"error\": str(e),\n",
    "                })\n",
    "\n",
    "print(f\"\\n✓ Testing complete! Generated {len(results)} results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Create output directory in Google Drive\n",
    "output_dir = f\"/content/drive/MyDrive/motivation_vectors_results/{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save full results as JSON\n",
    "results_path = os.path.join(output_dir, \"full_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Full results saved to: {results_path}\")\n",
    "\n",
    "# Save a summary CSV for easier analysis\n",
    "import csv\n",
    "\n",
    "summary_path = os.path.join(output_dir, \"summary.csv\")\n",
    "with open(summary_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        \"question_index\", \"question\", \"persona\", \"strength\", \"response_preview\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    for result in results:\n",
    "        if \"response\" in result:\n",
    "            writer.writerow({\n",
    "                \"question_index\": result[\"question_index\"],\n",
    "                \"question\": result[\"question\"],\n",
    "                \"persona\": result[\"persona\"],\n",
    "                \"strength\": result[\"strength\"],\n",
    "                \"response_preview\": result[\"response\"][:200] + \"...\" if len(result[\"response\"]) > 200 else result[\"response\"],\n",
    "            })\n",
    "\n",
    "print(f\"✓ Summary saved to: {summary_path}\")\n",
    "print(f\"\\n✓ All results saved to Google Drive at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few interesting comparisons\n",
    "print(\"\\n=== Sample Results ===\\n\")\n",
    "\n",
    "# Show first question with different personas at strength 0.7\n",
    "sample_question = motivation_test_questions[0]\n",
    "print(f\"Question: {sample_question}\\n\")\n",
    "\n",
    "for persona in motivation_vectors:\n",
    "    matching_results = [\n",
    "        r for r in results \n",
    "        if r.get(\"question\") == sample_question \n",
    "        and r.get(\"persona\") == persona \n",
    "        and r.get(\"strength\") == 0.7\n",
    "    ]\n",
    "    if matching_results:\n",
    "        result = matching_results[0]\n",
    "        print(f\"\\n[{persona} @ 0.7]\")\n",
    "        print(result.get(\"response\", \"No response\")[:300])\n",
    "        if len(result.get(\"response\", \"\")) > 300:\n",
    "            print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit-Based Multiple Choice Evaluation\n",
    "\n",
    "Fast evaluation using logits instead of generation. Useful for A/B/C/D questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logit evaluator\n",
    "from motivation_vectors.logit_evaluator import (\n",
    "    create_evaluator,\n",
    "    evaluate_with_control_vector,\n",
    "    compare_with_and_without_vector\n",
    ")\n",
    "\n",
    "# Create evaluator instance\n",
    "evaluator = create_evaluator(model, tokenizer)\n",
    "\n",
    "print(\"✓ Logit evaluator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Multiple Choice Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example multiple choice question\n",
    "question = \"\"\"When facing a difficult challenge, the best approach is to:\n",
    "A) Give up if it seems too hard\n",
    "B) Break it down into smaller, manageable steps\n",
    "C) Wait for someone else to solve it\n",
    "D) Avoid thinking about it\"\"\"\n",
    "\n",
    "# Evaluate without any control vector (baseline)\n",
    "result = evaluator.evaluate_multiple_choice(question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"\\nAnswer Probabilities:\")\n",
    "for choice, prob in result['probabilities'].items():\n",
    "    print(f\"  {choice}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTop Choice: {result['top_choice']} with {result['top_probability']:.4f} probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Evaluate with Control Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a motivation vector and see how it affects the choice\n",
    "motivation_question = \"\"\"I'm working on a project and hit a roadblock. I should:\n",
    "A) Give up and try something easier\n",
    "B) Take a break and come back with fresh perspective\n",
    "C) Push through no matter how tired I am\n",
    "D) Ask someone to do it for me\"\"\"\n",
    "\n",
    "# Compare baseline vs with motivation vector\n",
    "comparison = compare_with_and_without_vector(\n",
    "    evaluator,\n",
    "    motivation_question,\n",
    "    vec(\"extreme motivation and resilience\"),\n",
    "    strength=0.8\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE (No Vector)\")\n",
    "print(\"=\" * 60)\n",
    "for choice, prob in comparison['baseline']['probabilities'].items():\n",
    "    print(f\"  {choice}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "print(f\"\\nTop: {comparison['baseline']['top_choice']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WITH VECTOR: 'extreme motivation and resilience' * 0.8\")\n",
    "print(\"=\" * 60)\n",
    "for choice, prob in comparison['with_vector']['probabilities'].items():\n",
    "    shift = comparison['probability_shift'][choice]\n",
    "    arrow = \"↑\" if shift > 0 else \"↓\" if shift < 0 else \"→\"\n",
    "    print(f\"  {choice}: {prob:.4f} ({prob*100:.2f}%) {arrow} {abs(shift):.4f}\")\n",
    "print(f\"\\nTop: {comparison['with_vector']['top_choice']}\")\n",
    "\n",
    "if comparison['choice_changed']:\n",
    "    print(\"\\n⚠️  Choice CHANGED due to control vector!\")\n",
    "else:\n",
    "    print(\"\\n✓ Choice remained the same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Batch Evaluation Across Different Vector Strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how different vector strengths affect choices\n",
    "test_question = \"\"\"After experiencing failure, I believe:\n",
    "A) I'm just not good at this\n",
    "B) I can learn and improve with effort\n",
    "C) Success is mostly about luck\n",
    "D) I should find something I'm naturally good at\"\"\"\n",
    "\n",
    "strengths_to_test = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "persona = \"growth mindset and learning from failure\"\n",
    "\n",
    "print(f\"Testing: '{persona}'\")\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "\n",
    "results_by_strength = []\n",
    "\n",
    "for strength in strengths_to_test:\n",
    "    result = evaluate_with_control_vector(\n",
    "        evaluator,\n",
    "        test_question,\n",
    "        vec(persona) if strength > 0 else None,\n",
    "        strength\n",
    "    )\n",
    "    results_by_strength.append({\n",
    "        'strength': strength,\n",
    "        'top_choice': result['top_choice'],\n",
    "        'top_prob': result['top_probability'],\n",
    "        'probabilities': result['probabilities']\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Strength':<10} {'Top':<5} {'Prob':<8} {'A':<8} {'B':<8} {'C':<8} {'D':<8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for res in results_by_strength:\n",
    "    probs = res['probabilities']\n",
    "    print(\n",
    "        f\"{res['strength']:<10.1f} \"\n",
    "        f\"{res['top_choice']:<5} \"\n",
    "        f\"{res['top_prob']:<8.3f} \"\n",
    "        f\"{probs.get('A', 0):<8.3f} \"\n",
    "        f\"{probs.get('B', 0):<8.3f} \"\n",
    "        f\"{probs.get('C', 0):<8.3f} \"\n",
    "        f\"{probs.get('D', 0):<8.3f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
